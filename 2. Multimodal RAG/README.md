# ğŸ“š Multimodal RAG (Retrieval-Augmented Generation)

A **production-ready Multimodal RAG system** that combines **text + image understanding** to provide context-aware answers using external data sources such as **PDFs, TXT files, and images**.

This project demonstrates how to:

* Ingest multimodal documents (text + images)
* Chunk & embed content using **free HuggingFace embeddings**
* Store embeddings in **FAISS** (vector database)
* Query via **LangChain Multimodal Retrieval Chains**
* Use **GPT-4o** for natural language + multimodal reasoning

---

## ğŸš€ Features

âœ… **Multimodal Retrieval** (text + images)
âœ… **Free Embeddings** via HuggingFace (`all-MiniLM-L6-v2` for text, `clip-ViT-B-32` for images)
âœ… **FAISS Vector Database** for fast similarity search
âœ… **PDF, TXT, and Image Support** for knowledge ingestion
âœ… **Naive RAG & Multimodal RAG pipelines**
âœ… **Configurable project layout with `config.yaml`**
âœ… **Production-ready modular codebase**

---

## ğŸ—ï¸ Project Structure

```
multimodal-rag/
â”‚â”€â”€ data/                   # External raw files (PDFs, TXTs, Images)
â”‚â”€â”€ faiss_index/            # Serialized FAISS vector store
â”‚â”€â”€ src/
â”‚   â”œâ”€â”€ indexer.py          # Build multimodal index (text + images)
â”‚   â”œâ”€â”€ rag_chain.py        # Multimodal RAG pipeline
â”‚   â”œâ”€â”€ utils.py            # Helper functions (chunking, preprocessing, embeddings)
â”‚   â”œâ”€â”€ config.yaml         # Configurations
â”‚â”€â”€ app.py                  # Streamlit UI for querying
â”‚â”€â”€ requirements.txt        # Python dependencies
â”‚â”€â”€ README.md               # Project documentation
```

---

## âš™ï¸ Tech Stack

* **Python 3.10+**
* **LangChain** â†’ RAG pipeline & multimodal chain
* **FAISS** â†’ Vector database for retrieval
* **HuggingFace Transformers** â†’ Free embeddings (text + image)

  * `sentence-transformers/all-MiniLM-L6-v2` (text)
  * `openai/clip-vit-base-patch32` (images)
* **PyPDF2** â†’ PDF parsing
* **Pillow (PIL)** â†’ Image processing
* **Streamlit** â†’ Web UI for interaction
* **dotenv / YAML** â†’ Config management

---

## ğŸ”§ Setup Instructions

### 1ï¸âƒ£ Clone Repository

```bash
git clone https://github.com/yourusername/multimodal-rag.git
cd multimodal-rag
```

### 2ï¸âƒ£ Create Virtual Environment

```bash
python -m venv rag_env
source rag_env/bin/activate   # Mac/Linux
rag_env\Scripts\activate      # Windows
```

### 3ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

### 4ï¸âƒ£ Configure Settings

Edit `src/config.yaml` to specify:

```yaml
faiss:
  index_path: "faiss_index"
chunk_size: 500
chunk_overlap: 50
```

### 5ï¸âƒ£ Add Data

Place your files in `data/` folder:

* `.pdf` â†’ research papers, docs
* `.txt` â†’ raw text files
* `.jpg/.png` â†’ images

### 6ï¸âƒ£ Build Index

```bash
python src/indexer.py
```

### 7ï¸âƒ£ Run Multimodal RAG

```bash
python src/rag_chain.py
```

### 8ï¸âƒ£ Run Streamlit App

```bash
streamlit run app.py
```

---

## ğŸ“Š Example Queries

**Text Query**

```text
"Summarize the PDF report on climate change."
```

**Image Query**

```text
"Show me Einsteinâ€™s handwritten notes."
```

**Mixed Query**

```text
"Find the diagram from the physics textbook and explain it."
```

---

## ğŸ› ï¸ Future Enhancements

* [ ] Add **Whisper** for audio-to-text RAG ingestion
* [ ] Integrate **Milvus / Weaviate** for scalable vector search
* [ ] Add **Fine-tuning adapters** for domain-specific multimodal embeddings
* [ ] Deploy with **Docker + FastAPI** backend

---

## ğŸ‘¨â€ğŸ’» Author

**Parag Verma** â€“ Data Scientist | Generative AI | Multimodal RAG Enthusiast